{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/Applications/ML projects/Song Lyrics/Dataset - 2/archive/csv'\n",
    "csv_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 54981.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Title</th>\n",
       "      <th>Album</th>\n",
       "      <th>Year</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>New Rules</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2017-06-02</td>\n",
       "      <td>one one one one one   talkin' in my sleep at n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Don’t Start Now</td>\n",
       "      <td>Future Nostalgia</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>if you don't wanna see me   did a full 80 craz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>IDGAF</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2017-06-02</td>\n",
       "      <td>you call me all friendly tellin' me how much y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Blow Your Mind (Mwah)</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>i know it's hot i know we've got something tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Be the One</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2015-10-30</td>\n",
       "      <td>i see the moon i see the moon i see the moon o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Artist                  Title             Album    Year  \\\n",
       "0         0.0  Dua Lipa              New Rules          Dua Lipa  2017.0   \n",
       "1         1.0  Dua Lipa        Don’t Start Now  Future Nostalgia  2019.0   \n",
       "2         2.0  Dua Lipa                  IDGAF          Dua Lipa  2017.0   \n",
       "3         3.0  Dua Lipa  Blow Your Mind (Mwah)          Dua Lipa  2016.0   \n",
       "4         4.0  Dua Lipa             Be the One          Dua Lipa  2015.0   \n",
       "\n",
       "         Date                                              Lyric  \n",
       "0  2017-06-02  one one one one one   talkin' in my sleep at n...  \n",
       "1  2019-11-01  if you don't wanna see me   did a full 80 craz...  \n",
       "2  2017-06-02  you call me all friendly tellin' me how much y...  \n",
       "3  2016-08-26  i know it's hot i know we've got something tha...  \n",
       "4  2015-10-30  i see the moon i see the moon i see the moon o...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for CSV_NAME in tqdm(listdir(csv_path)):\n",
    "    CSV_PATH = join(csv_path, CSV_NAME)\n",
    "    csv_files.append(CSV_PATH)\n",
    "\n",
    "dataframe = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True).dropna()\n",
    "dataframe[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = dataframe['Lyric'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = []\n",
    "for lyric in lyrics:\n",
    "    all_lyrics.append(lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3207"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(lyric.split()) for lyric in all_lyrics)\n",
    "max_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(lyrics, max_words):\n",
    "    hashmap1 = {}\n",
    "    for lyric in lyrics:\n",
    "        for word in lyric:\n",
    "            if word not in hashmap1:\n",
    "                hashmap1[word] = 1\n",
    "            else:\n",
    "                hashmap1[word] += 1\n",
    "\n",
    "    max_frequency = max(hashmap1.values()) + 1\n",
    "    ranks = []\n",
    "    hashmap2 = {}\n",
    "    for key in hashmap1.keys():\n",
    "        rank = max_frequency - hashmap1[key]\n",
    "        hashmap2[key] = rank\n",
    "        ranks.append(rank)\n",
    "    ranks = sorted(ranks)\n",
    "    threshold_rank = ranks[max_words]\n",
    "\n",
    "    hashmap2Keys = hashmap2.keys()\n",
    "    for key in hashmap2Keys:\n",
    "        if hashmap2[key] >= threshold_rank:\n",
    "            ranks.remove(hashmap2[key])\n",
    "            hashmap2[key] = -1\n",
    "    ranks = sorted(ranks)\n",
    "\n",
    "    tokenizer = {}\n",
    "    for key in hashmap2.keys():\n",
    "        if hashmap2[key] != -1:\n",
    "            rank = ranks.index(hashmap2[key]) + 1\n",
    "            tokenizer[key] = rank\n",
    "        \n",
    "    tokenized_lyrics = []\n",
    "    for lyric in lyrics:\n",
    "        temp = []\n",
    "        for word in lyric:\n",
    "            if word in tokenizer:\n",
    "                temp.append(tokenizer[word])\n",
    "        tokenized_lyrics.append(temp)\n",
    "\n",
    "    return tokenized_lyrics, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pow\n",
    "\n",
    "def normalization(lyrics, hashmap):\n",
    "    ranks = hashmap.values()\n",
    "    n = len(ranks)\n",
    "\n",
    "    rank_mean = sum(ranks) / n\n",
    "    rank_sub2 = []\n",
    "    for rank in ranks:\n",
    "        rank_sub2.append(pow(rank - rank_mean, 2))\n",
    "    rank_sub2sum = sum(rank_sub2)\n",
    "    rank_variance = rank_sub2sum / n\n",
    "    rank_sd = pow(rank_variance, 0.5)\n",
    "\n",
    "    normalized_lyrics = []\n",
    "    for lyric in lyrics:\n",
    "        temp = []\n",
    "        for word in lyric:\n",
    "            rank = word\n",
    "            n1 = (rank - rank_mean) / rank_sd\n",
    "            n2 = -1 / (n1 + 1)\n",
    "            temp.append(n2)\n",
    "        normalized_lyrics.append(temp)\n",
    "\n",
    "    return normalized_lyrics, rank_mean, rank_sd, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(lyrics, max_words, strip):\n",
    "    split_lyrics = []\n",
    "\n",
    "    for lyric in lyrics:\n",
    "        split_lyrics.append(lyric.split(' '))\n",
    "\n",
    "    tokenized_lyrics, tokenizer = tokenization(split_lyrics, max_words)\n",
    "    normalized_lyrics, rank_mean, rank_sd = normalization(tokenized_lyrics, tokenizer)\n",
    "    \n",
    "    normalized_lyrics = sorted(normalized_lyrics, key=len)\n",
    "    length = len(normalized_lyrics)\n",
    "    lower_strip = int(strip * length)\n",
    "    higher_strip = length - lower_strip\n",
    "    LYRICS = normalized_lyrics[lower_strip: higher_strip]\n",
    "\n",
    "    return LYRICS, rank_mean, rank_sd, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRICS, MEAN, STANDARD_DEVIATION, TOKENIZER = preprocess(lyrics, 5000, 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def prepare_data(LYRICS, input_length, output_length):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for lyric in LYRICS:\n",
    "        X.append(lyric[:input_length])\n",
    "        y.append(lyric[input_length: input_length + output_length])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    x_train, x_test, y_train, y_test = np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = prepare_data(LYRICS, 150, 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_MODEL(rnn_units, time_steps, features, output_steps):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(rnn_units, input_shape=(time_steps, features)))\n",
    "    model.add(Dense(output_steps, activation='tanh'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = x_train.shape[1]\n",
    "features = x_train.shape[2]\n",
    "output_steps = y_train.shape[1]\n",
    "rnn_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 256)               264192    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 50)                12850     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 277,042\n",
      "Trainable params: 277,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN_MODEL(rnn_units, time_steps, features, output_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 378.1096 - accuracy: 0.0182\n",
      "Epoch 1: loss improved from inf to 378.10962, saving model to weights-improvement-01-378.1096.hdf5\n",
      "49/49 [==============================] - 7s 100ms/step - loss: 378.1096 - accuracy: 0.0182\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 321.9419 - accuracy: 0.0201\n",
      "Epoch 2: loss improved from 378.10962 to 321.94189, saving model to weights-improvement-02-321.9419.hdf5\n",
      "49/49 [==============================] - 5s 100ms/step - loss: 321.9419 - accuracy: 0.0201\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 317.4323 - accuracy: 0.0208\n",
      "Epoch 3: loss improved from 321.94189 to 317.43234, saving model to weights-improvement-03-317.4323.hdf5\n",
      "49/49 [==============================] - 5s 100ms/step - loss: 317.4323 - accuracy: 0.0208\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 329.4511 - accuracy: 0.0149\n",
      "Epoch 4: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 99ms/step - loss: 329.4511 - accuracy: 0.0149\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 323.5001 - accuracy: 0.0143\n",
      "Epoch 5: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 99ms/step - loss: 323.5001 - accuracy: 0.0143\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 330.6030 - accuracy: 0.0143\n",
      "Epoch 6: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 107ms/step - loss: 330.6030 - accuracy: 0.0143\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 333.4044 - accuracy: 0.0136\n",
      "Epoch 7: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 102ms/step - loss: 333.4044 - accuracy: 0.0136\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 333.2584 - accuracy: 0.0214\n",
      "Epoch 8: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 103ms/step - loss: 333.2584 - accuracy: 0.0214\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 333.0682 - accuracy: 0.0214\n",
      "Epoch 9: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 6s 114ms/step - loss: 333.0682 - accuracy: 0.0214\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 333.3099 - accuracy: 0.0214\n",
      "Epoch 10: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 99ms/step - loss: 333.3099 - accuracy: 0.0214\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 332.0991 - accuracy: 0.0214\n",
      "Epoch 11: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 106ms/step - loss: 332.0991 - accuracy: 0.0214\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 332.2020 - accuracy: 0.0214\n",
      "Epoch 12: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 107ms/step - loss: 332.2020 - accuracy: 0.0214\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 333.1792 - accuracy: 0.0214\n",
      "Epoch 13: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 102ms/step - loss: 333.1792 - accuracy: 0.0214\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 332.2016 - accuracy: 0.0214\n",
      "Epoch 14: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 101ms/step - loss: 332.2016 - accuracy: 0.0214\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 332.0876 - accuracy: 0.0214\n",
      "Epoch 15: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 99ms/step - loss: 332.0876 - accuracy: 0.0214\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 331.5666 - accuracy: 0.0214\n",
      "Epoch 16: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 97ms/step - loss: 331.5666 - accuracy: 0.0214\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 331.0345 - accuracy: 0.0214\n",
      "Epoch 17: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 103ms/step - loss: 331.0345 - accuracy: 0.0214\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 331.0414 - accuracy: 0.0214\n",
      "Epoch 18: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 103ms/step - loss: 331.0414 - accuracy: 0.0214\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 331.0664 - accuracy: 0.0214\n",
      "Epoch 19: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 5s 104ms/step - loss: 331.0664 - accuracy: 0.0214\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - ETA: 0s - loss: 330.7680 - accuracy: 0.0214\n",
      "Epoch 20: loss did not improve from 317.43234\n",
      "49/49 [==============================] - 6s 118ms/step - loss: 330.7680 - accuracy: 0.0214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x148253550>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 150, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 150, 1), dtype=tf.float32, name='lstm_5_input'), name='lstm_5_input', description=\"created by layer 'lstm_5_input'\"), but it was called on an input with incompatible shape (None, 1, 1).\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.07945235, 0.00763285, 0.07255211, ..., 0.10205323, 0.06520914,\n",
       "        0.04087023],\n",
       "       [0.05987512, 0.00835881, 0.05648387, ..., 0.07536648, 0.05511349,\n",
       "        0.03340356],\n",
       "       [0.08481648, 0.00743614, 0.07697365, ..., 0.1093156 , 0.0679851 ,\n",
       "        0.04301887],\n",
       "       ...,\n",
       "       [0.05987512, 0.00835881, 0.05648387, ..., 0.07536648, 0.05511349,\n",
       "        0.03340356],\n",
       "       [0.06125525, 0.00830721, 0.05761297, ..., 0.07725696, 0.05582319,\n",
       "        0.03391042],\n",
       "       [0.05987512, 0.00835881, 0.05648387, ..., 0.07536648, 0.05511349,\n",
       "        0.03340356]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.37312847,  1.33498754,  1.36773859,  1.33626744,  1.3375498 ,\n",
       "        1.3375498 ,  1.34012191, -2.13456708,  1.72916934,  1.34270392,\n",
       "        1.35973264,  1.98815926,  1.35973264,  1.98815926,  1.37042823,\n",
       "        1.37042823,  1.3375498 ,  1.3375498 ,  1.33626744,  1.36505949,\n",
       "        1.35051003,  1.33498754,  1.77768936,  1.36908209,  1.33626744,\n",
       "        1.43534788,  1.56246353,  1.35973264,  1.47953391,  1.36908209,\n",
       "        1.33626744,  1.33498754,  2.63454172,  1.43534788,  1.37177702,\n",
       "        1.59829813,  1.42945953,  1.45939454,  1.37042823,  1.33498754,\n",
       "        1.36505949,  1.35051003,  1.33626744,  1.40922537,  1.34141167,\n",
       "        1.46399327,  1.39094277,  1.36639773,  1.39791812,  1.42653343])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
