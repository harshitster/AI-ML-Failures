{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/Applications/ML projects/Song Lyrics/Dataset - 2/archive/csv'\n",
    "csv_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 112347.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Title</th>\n",
       "      <th>Album</th>\n",
       "      <th>Year</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>New Rules</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2017-06-02</td>\n",
       "      <td>one one one one one   talkin' in my sleep at n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Don’t Start Now</td>\n",
       "      <td>Future Nostalgia</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>if you don't wanna see me   did a full 80 craz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>IDGAF</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2017-06-02</td>\n",
       "      <td>you call me all friendly tellin' me how much y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Blow Your Mind (Mwah)</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>i know it's hot i know we've got something tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>Be the One</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2015-10-30</td>\n",
       "      <td>i see the moon i see the moon i see the moon o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Artist                  Title             Album    Year  \\\n",
       "0         0.0  Dua Lipa              New Rules          Dua Lipa  2017.0   \n",
       "1         1.0  Dua Lipa        Don’t Start Now  Future Nostalgia  2019.0   \n",
       "2         2.0  Dua Lipa                  IDGAF          Dua Lipa  2017.0   \n",
       "3         3.0  Dua Lipa  Blow Your Mind (Mwah)          Dua Lipa  2016.0   \n",
       "4         4.0  Dua Lipa             Be the One          Dua Lipa  2015.0   \n",
       "\n",
       "         Date                                              Lyric  \n",
       "0  2017-06-02  one one one one one   talkin' in my sleep at n...  \n",
       "1  2019-11-01  if you don't wanna see me   did a full 80 craz...  \n",
       "2  2017-06-02  you call me all friendly tellin' me how much y...  \n",
       "3  2016-08-26  i know it's hot i know we've got something tha...  \n",
       "4  2015-10-30  i see the moon i see the moon i see the moon o...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for CSV_NAME in tqdm(listdir(csv_path)):\n",
    "    CSV_PATH = join(csv_path, CSV_NAME)\n",
    "    csv_files.append(CSV_PATH)\n",
    "\n",
    "dataframe = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True).dropna()\n",
    "dataframe[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = dataframe['Lyric'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(lyrics, max_words):\n",
    "    hashmap1 = {}\n",
    "    for lyric in lyrics:\n",
    "        for word in lyric:\n",
    "            if word not in hashmap1:\n",
    "                hashmap1[word] = 1\n",
    "            else:\n",
    "                hashmap1[word] += 1\n",
    "\n",
    "    max_frequency = max(hashmap1.values()) + 1\n",
    "    ranks = []\n",
    "    hashmap2 = {}\n",
    "    for key in hashmap1.keys():\n",
    "        rank = max_frequency - hashmap1[key]\n",
    "        hashmap2[key] = rank\n",
    "        ranks.append(rank)\n",
    "    ranks = sorted(ranks)\n",
    "    threshold_rank = ranks[max_words]\n",
    "\n",
    "    hashmap2Keys = hashmap2.keys()\n",
    "    for key in hashmap2Keys:\n",
    "        if hashmap2[key] >= threshold_rank:\n",
    "            ranks.remove(hashmap2[key])\n",
    "            hashmap2[key] = -1\n",
    "    ranks = sorted(ranks)\n",
    "\n",
    "    tokenizer = {}\n",
    "    for key in hashmap2.keys():\n",
    "        if hashmap2[key] != -1:\n",
    "            rank = ranks.index(hashmap2[key]) + 1\n",
    "            tokenizer[key] = rank\n",
    "        \n",
    "    tokenized_lyrics = []\n",
    "    for lyric in lyrics:\n",
    "        temp = []\n",
    "        for word in lyric:\n",
    "            if word in tokenizer:\n",
    "                temp.append(tokenizer[word])\n",
    "        tokenized_lyrics.append(temp)\n",
    "\n",
    "    return tokenized_lyrics, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(tokenized_lyrics, tokenizer):\n",
    "    max_rank = float(max(tokenizer.values()))\n",
    "    for i in range(len(tokenized_lyrics)):\n",
    "        for j in range(len(tokenized_lyrics[i])):\n",
    "            tokenized_lyrics[i][j] = tokenized_lyrics[i][j] / max_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(lyrics, max_words, strip):\n",
    "    split_lyrics = []\n",
    "\n",
    "    for lyric in lyrics:\n",
    "        split_lyrics.append(lyric.split(' '))\n",
    "\n",
    "    tokenized_lyrics, tokenizer = tokenization(split_lyrics, max_words)\n",
    "    normalization(tokenized_lyrics, tokenizer)\n",
    "    \n",
    "    normalized_lyrics = sorted(tokenized_lyrics, key=len)\n",
    "    length = len(normalized_lyrics)\n",
    "    lower_strip = int(strip * length)\n",
    "    higher_strip = length - lower_strip\n",
    "    LYRICS = normalized_lyrics[lower_strip: higher_strip]\n",
    "\n",
    "    return LYRICS, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRICS, TOKENIZER = preprocess(lyrics, 5000, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def prepare_data(LYRICS, input_length, output_length):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for lyric in LYRICS:\n",
    "        X.append(lyric[:input_length])\n",
    "        y.append(lyric[input_length: input_length + output_length])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    x_train, x_test, y_train, y_test = np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = prepare_data(LYRICS, 150, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_MODEL(rnn_units, time_steps, features, output_steps):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(rnn_units, input_shape=(time_steps, features), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(rnn_units))\n",
    "    model.add(Dense(output_steps, activation='relu'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = x_train.shape[1]\n",
    "features = x_train.shape[2]\n",
    "output_steps = y_train.shape[1]\n",
    "rnn_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 150, 256)          264192    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 150, 256)          0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                12850     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 802,354\n",
      "Trainable params: 802,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN_MODEL(rnn_units, time_steps, features, output_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
